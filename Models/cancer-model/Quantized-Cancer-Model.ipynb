{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import the libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q tensorflow-model-optimization","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\r\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nfrom glob import glob\nimport seaborn as sns\nimport pprint as pp\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import resample\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_model_optimization as tfmot\nimport keras.utils\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\n\nimport itertools\n\nimport cv2\nfrom PIL import Image\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"base_skin_dir = os.path.join('..', 'input/skin-cancer-mnist-ham10000')\n\n# Merging images from both folders HAM10000_images_part1.zip and HAM10000_images_part2.zip into one dictionary\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n                     for x in glob(os.path.join(base_skin_dir, '*', '*.jpg'))}\n\n# This dictionary is useful for displaying more human-friendly labels later on\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'Melanoma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata.csv'))\n\n# Creating New Columns for better readability\ndata['path'] = data['image_id'].map(imageid_path_dict.get)\ndata['cell_type'] = data['dx'].map(lesion_type_dict.get) \ndata['cell_type_idx'] = pd.Categorical(data['cell_type']).codes\n\ndata.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"     lesion_id      image_id   dx dx_type   age   sex localization  \\\n0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n\n                                                path  \\\n0  ../input/skin-cancer-mnist-ham10000/HAM10000_i...   \n1  ../input/skin-cancer-mnist-ham10000/HAM10000_i...   \n2  ../input/skin-cancer-mnist-ham10000/HAM10000_i...   \n3  ../input/skin-cancer-mnist-ham10000/HAM10000_i...   \n4  ../input/skin-cancer-mnist-ham10000/HAM10000_i...   \n\n                        cell_type  cell_type_idx  \n0  Benign keratosis-like lesions               2  \n1  Benign keratosis-like lesions               2  \n2  Benign keratosis-like lesions               2  \n3  Benign keratosis-like lesions               2  \n4  Benign keratosis-like lesions               2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>cell_type_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/skin-cancer-mnist-ham10000/HAM10000_i...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/skin-cancer-mnist-ham10000/HAM10000_i...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/skin-cancer-mnist-ham10000/HAM10000_i...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>../input/skin-cancer-mnist-ham10000/HAM10000_i...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n      <td>../input/skin-cancer-mnist-ham10000/HAM10000_i...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(1, 1, figsize = (10, 5))\ndata['cell_type'].value_counts().plot(kind='bar', ax=ax1, color='teal', alpha=0.4)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7feb1d7c7c90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x360 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We can see that the dataset is highly imbalanced. The number of images for Melanocytic Levi is much more than those for the other classes. So let's resample/upsample each class to 7000 instances. The code below returns a balanced dataset, in which each class has 7000 instances. Of course this leads to heavily redundant data in categories with originally very few instances."},{"metadata":{"trusted":true},"cell_type":"code","source":"def balanced_dataset(df):\n    df_balanced = pd.DataFrame()\n    #df = pd.DataFrame()\n    \n    for cat in df['cell_type_idx'].unique():\n        temp = resample(df[df['cell_type_idx'] == cat], \n                        replace=True,     # sample with replacement\n                        n_samples=7000,   # to match majority class\n                        random_state=123) # reproducible results\n\n        # Combine majority class with upsampled minority class\n        df_balanced = pd.concat([df_balanced, temp])\n \n    df_balanced['cell_type'].value_counts()\n\n    return df_balanced","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rescaling the images to (128,128,3) and standardizing (division by 255)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef load_img_data(size, df, balanced=False):\n    #first we should normalize the image from 0-255 to 0-1\n    \n    img_h, img_w = size, size\n    imgs = []\n    \n    if balanced:\n        df = balanced_dataset(df)\n    \n    image_paths = list(df['path'])\n\n    for i in tqdm(range(len(image_paths))):\n        img = cv2.imread(image_paths[i])\n        img = cv2.resize(img, (img_h, img_w))\n        img = img.astype(np.float32) / 255.\n        #img = np.asarray(Image.open(image_paths[i]).resize((size,size)))\n        imgs.append(img)\n\n    imgs = np.stack(imgs, axis=0)\n    print(imgs.shape)\n    #imgs = imgs.astype(np.float32) / 255.\n    \n    return imgs, df['cell_type_idx'].values\n\nimgs, target   = load_img_data(128, data, balanced=False)","execution_count":7,"outputs":[{"output_type":"stream","text":"100%|██████████| 10015/10015 [02:06<00:00, 78.93it/s]\n","name":"stderr"},{"output_type":"stream","text":"(10015, 128, 128, 3)\nCPU times: user 1min 44s, sys: 5.74 s, total: 1min 50s\nWall time: 2min 7s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Building The model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(imgs, target, test_size=0.20)\nx_train, x_val, y_train, y_val = train_test_split(imgs, target, test_size=0.05)\n\ntrain_val_test = (x_train, y_train, x_val, y_val, x_test, y_test)\n\n[x_train.shape, x_val.shape, x_test.shape]","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"[(9514, 128, 128, 3), (501, 128, 128, 3), (2003, 128, 128, 3)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"From the various different architectures mentioned ResNet101v2 worked best.\nThe training is done in 2 phases:\n\n* Phase 1: \n           Only the new dense layers added on top of the base model is trained and the base model's layers\n           remain frozen. This makes the output of the CNN convolutions remain stable and allows the dense\n           layers to learn to classify the extracted features to classes.\n          \n* Phase 2: \n           Here we additionally fine tune the the entire model to further increase the predictive accuracy\n           of the network. A lower learning rate is used to prevent too drastic changes to the feature extracters.\n         \nIf the full CNN would be trained immediately, i.e. skipping phase 1, the completely untrained dense\nlayers would initially create close-to-random predictions leading to a high loss. This loss would\nthen be back-propagated through the whole CNN and likely “break” the already well-trained feature detectors.\n\n         \n* In phase 2 we can also train selected layers of the base model but training the entire model resulted better. \n* Data augmentation did not result in better results probably because of already upsampled data. So there was no\n  need, and in fact this resulted in better acc.    \n* Custom weights for different classes was tried but resulted in weaker training progress so it has been\n  removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Tuner(object):\n\n    def __init__(self, data, architecture, hidden_layers, classes, epochs, batch_size):\n        self.input_shape = data[0][0].shape\n\n        self.base_arch = architecture\n        self.nn = self.download_network()\n        self.nn.trainable = False\n\n        self.hidden_layers = hidden_layers\n        \n        self.classes = classes\n\n        self.trainX = data[0]\n        self.trainY = data[1]\n        self.valX = data[2]\n        self.valY = data[3]\n        self.testX = data[4]\n        self.testY = data[5]\n\n        self.EPOCHS = epochs\n        self.BATCH_SIZE = batch_size\n        \n        self.model = self.build()\n        self.train_generator = self.data_augmentation()\n        self.predictions = None\n        self.score = None\n\n        self.best_weights = None\n        \n    def download_network(self):\n        #Download the requested CNN with imagenet weights\n\n        nn = None\n\n        if self.base_arch == 'VGG16':\n            nn = tf.keras.applications.VGG16(weights='imagenet', include_top=False, \n                                             input_shape=self.input_shape)\n        elif self.base_arch == 'VGG19':\n            nn = tf.keras.applications.VGG19(weights='imagenet', include_top=False, \n                                             input_shape=self.input_shape)\n        elif self.base_arch == 'InceptionV3':\n            nn = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n        elif self.base_arch == 'Xception':\n            nn = tf.keras.applications.Xception(weights='imagenet', include_top=False, \n                                                input_shape=self.input_shape)\n        elif self.base_arch == 'DenseNet121':\n            nn = tf.keras.applications.DenseNet121(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n        elif self.base_arch == 'DenseNet201':\n            nn = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n        elif self.base_arch == 'ResNet101V2':\n            nn = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n        elif self.base_arch == 'MobileNet':\n            nn = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, \n                                                 input_shape=self.input_shape)\n        elif self.base_arch == 'MobileNetV2':\n            nn = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n        elif self.base_arch == 'EfficientNetB5':\n            nn = tf.keras.applications.EfficientNetB5(weights='imagenet', include_top=False, \n                                                   input_shape=self.input_shape)\n\n        return nn\n    \n    def run(self):\n        '''\n        Main driver for Learner object\n        '''\n        self.fine_tune()\n        #self.load_weights(self.best_weights)\n        #self.predict()\n        \n    def build(self):\n        '''\n        Build model. Add Dense layer to topless base CNN.\n        '''\n\n        model = tf.keras.models.Sequential()\n        model.add(self.nn)\n        model.add(tf.keras.layers.Flatten())\n        model.add(tf.keras.layers.Dropout(0.25))\n        \n        for layer in self.hidden_layers:\n            model.add(tf.keras.layers.Dense(layer, activation='relu'))\n            model.add(tf.keras.layers.BatchNormalization())\n            model.add(tf.keras.layers.Dropout(0.46))  \n\n        model.add(tf.keras.layers.Dense(self.classes, activation='softmax'))\n        \n        print (model.summary())\n\n        return model\n    \n    def load_weights(self, name):\n        #Load the best checkpointed weights.\n        \n        print('\\nLoading best accuracy weights.')\n        self.model.load_weights(name)\n        \n    def data_augmentation(self):\n        data_gen_args = dict(\n                rotation_range=10,\n                zoom_range=0.1,\n                shear_range=0.1,\n                width_shift_range=0.1, \n                height_shift_range=0.1,\n                horizontal_flip=True,\n                vertical_flip=True,\n            )\n\n        train_gen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)\n        train_generator = train_gen.flow(self.trainX, self.trainY, batch_size=self.BATCH_SIZE)\n\n        #print('\\nData augmentation with the following parameters:')\n        #pp.pprint(data_gen_args)\n\n        return train_generator\n    \n    \n    def fine_tune(self):\n        '''\n        Fine-tune network in 2 phases\n        '''\n\n        numTrainingSamples = self.trainX.shape[0]\n        numValidationSamples = self.valX.shape[0]\n\n        print (\"\\nPhase A - Training Fully Connected Layers\\n\")\n        self.model.compile(loss='sparse_categorical_crossentropy', \n                           optimizer=tf.keras.optimizers.Adam(lr=0.001), \n                           metrics=['accuracy'])\n\n        # Define checkpoint to save best Phase 1 weights\n        best_weights_ph1 = self.base_arch + \"_ph1_weights.hdf5\"\n        checkpoint = [tfmot.sparsity.keras.UpdatePruningStep(), \n                      tf.keras.callbacks.ModelCheckpoint(best_weights_ph1, monitor=\"val_loss\", \n                                                         mode=\"min\", save_best_only=True, verbose=1)]\n\n        \n        history = self.model.fit(\n            x_train, y_train,\n            #self.train_generator,\n            steps_per_epoch=numTrainingSamples // self.BATCH_SIZE,\n            epochs=self.EPOCHS,\n            validation_data=(self.valX, self.valY),\n            validation_steps=numValidationSamples // self.BATCH_SIZE,\n            #class_weight=self.get_class_weight(),\n            callbacks=checkpoint)\n        \n        # Store the best phase 1 accuracy\n        best_acc_ph1 = max(history.history[\"val_accuracy\"])\n        print('\\n\\nMax validation accuracy:', best_acc_ph1)\n\n        print('\\nRestoring best weights and predicting validation set.')\n        self.load_weights(best_weights_ph1)\n\n        # Make predictions based on best phase 1 weights\n        self.predict()\n\n        self.plot_loss(history, self.EPOCHS, '\\n Transfer Learning: ' + self.base_arch + ' Ph A')\n        \n        print (\"\\nPhase B  - Fine Tune all Layers \\n\")\n        # Set full original CNN as trainable\n        self.nn.trainable = True\n\n        self.model.compile(loss='sparse_categorical_crossentropy', \n                           optimizer=tf.keras.optimizers.Adam(lr=1e-5), \n                           metrics=['accuracy'])\n        \n        # Define checkpoint to save best Phase 2 weights\n        best_weights_ph2 = self.base_arch + \"_ph2_weights.hdf5\"\n        checkpoint = [tfmot.sparsity.keras.UpdatePruningStep(), \n                      tf.keras.callbacks.ModelCheckpoint(best_weights_ph2, monitor=\"val_loss\", mode=\"min\", \n                                                         save_best_only=True, verbose=1)]\n\n\n        # Fine-tune the full CNN + FC\n        history = self.model.fit(\n            x_train, y_train,\n            #self.train_generator,\n            steps_per_epoch=numTrainingSamples // self.BATCH_SIZE,\n            epochs=self.EPOCHS,\n            validation_data=(self.valX, self.valY),\n            validation_steps=numValidationSamples // self.BATCH_SIZE,\n            #class_weight=self.get_class_weight(),\n            callbacks=checkpoint)\n        \n        # Store the best phase 2 accuracy\n        best_acc_ph2 = max(history.history[\"val_accuracy\"])\n        print('\\n\\nMax validation accuracy:', best_acc_ph2)\n\n        # Only if Phase 2 fine-tuning resulted in a better accuracy than phase 1,\n        # restore best phase 2 weights and update Tuner predictions.\n        if best_acc_ph2 > best_acc_ph1:\n            print('\\nPhase 2 resulted in better accuracy than Phase 1.')\n            print('Restoring best weights of Ph2 and predicting validation set.')\n            self.load_weights(best_weights_ph2)\n            self.predict()\n\n        self.plot_loss(history, self.EPOCHS, '\\n Transfer Learning: ' + self.base_arch + ' Ph B')\n        \n    def predict(self):\n        '''\n        Get predictions and score for validation set.\n        '''\n        print('\\nPredicting test set classes.')\n        self.score = self.model.evaluate(self.testX, self.testY, verbose=0)\n        print('Test set score:', self.score)\n        self.predictions = self.model.predict(self.testX, batch_size=self.BATCH_SIZE)\n        print('Done')\n\n    def plot_loss(self, history, epochs, name):\n        print('\\n\\n')\n        plt.figure(figsize=(12,8))\n        plt.plot(np.arange(0, epochs), history.history[\"loss\"], label=\"train_loss\")\n        plt.plot(np.arange(0, epochs), history.history[\"val_loss\"], label=\"val_loss\")\n        plt.plot(np.arange(0, epochs), history.history[\"accuracy\"], label=\"train_acc\")\n        plt.plot(np.arange(0, epochs), history.history[\"val_accuracy\"], label=\"val_acc\")\n        plt.title(\"Training Loss and Accuracy - {}\".format(name))\n        plt.xlabel(\"Epoch #\")\n        plt.ylabel(\"Loss/Accuracy\")\n        plt.legend()\n        plt.show()\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nNET = 'ResNet101V2'\nHIDDEN_LAYERS = [512, 128]\n#HIDDEN_LAYERS = [256, 256, 64]\nCLASSES = len(set(target))\nBATCH_SIZE = 128\nEPOCHS = 15\n\ncrmodel = Tuner(train_val_test, NET, HIDDEN_LAYERS, CLASSES, EPOCHS, BATCH_SIZE)\ncrmodel.run()","execution_count":null,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresnet50v2 (Functional)      (None, 4, 4, 2048)        23564800  \n_________________________________________________________________\nflatten (Flatten)            (None, 32768)             0         \n_________________________________________________________________\ndropout (Dropout)            (None, 32768)             0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               16777728  \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 512)               2048      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               65664     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 128)               512       \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 7)                 903       \n=================================================================\nTotal params: 40,411,655\nTrainable params: 16,845,575\nNon-trainable params: 23,566,080\n_________________________________________________________________\nNone\n\nPhase A - Training Fully Connected Layers\n\nEpoch 1/15\n74/74 [==============================] - ETA: 0s - loss: 1.6351 - accuracy: 0.5513\nEpoch 00001: val_loss improved from inf to 1.09110, saving model to ResNet101V2_ph1_weights.hdf5\n74/74 [==============================] - 9s 128ms/step - loss: 1.6351 - accuracy: 0.5513 - val_loss: 1.0911 - val_accuracy: 0.7405\nEpoch 2/15\n74/74 [==============================] - ETA: 0s - loss: 1.0437 - accuracy: 0.7013\nEpoch 00002: val_loss improved from 1.09110 to 0.94841, saving model to ResNet101V2_ph1_weights.hdf5\n74/74 [==============================] - 7s 100ms/step - loss: 1.0437 - accuracy: 0.7013 - val_loss: 0.9484 - val_accuracy: 0.7285\nEpoch 3/15\n74/74 [==============================] - ETA: 0s - loss: 0.8385 - accuracy: 0.7457\nEpoch 00003: val_loss improved from 0.94841 to 0.85160, saving model to ResNet101V2_ph1_weights.hdf5\n74/74 [==============================] - 7s 97ms/step - loss: 0.8385 - accuracy: 0.7457 - val_loss: 0.8516 - val_accuracy: 0.7525\nEpoch 4/15\n74/74 [==============================] - ETA: 0s - loss: 0.7369 - accuracy: 0.7673\nEpoch 00004: val_loss did not improve from 0.85160\n74/74 [==============================] - 6s 83ms/step - loss: 0.7369 - accuracy: 0.7673 - val_loss: 0.8609 - val_accuracy: 0.7365\nEpoch 5/15\n74/74 [==============================] - ETA: 0s - loss: 0.6303 - accuracy: 0.7921\nEpoch 00005: val_loss improved from 0.85160 to 0.78335, saving model to ResNet101V2_ph1_weights.hdf5\n74/74 [==============================] - 7s 99ms/step - loss: 0.6303 - accuracy: 0.7921 - val_loss: 0.7834 - val_accuracy: 0.7665\nEpoch 6/15\n74/74 [==============================] - ETA: 0s - loss: 0.5713 - accuracy: 0.8127\nEpoch 00006: val_loss improved from 0.78335 to 0.76382, saving model to ResNet101V2_ph1_weights.hdf5\n74/74 [==============================] - 7s 95ms/step - loss: 0.5713 - accuracy: 0.8127 - val_loss: 0.7638 - val_accuracy: 0.7525\nEpoch 7/15\n74/74 [==============================] - ETA: 0s - loss: 0.5135 - accuracy: 0.8290\nEpoch 00007: val_loss did not improve from 0.76382\n74/74 [==============================] - 6s 83ms/step - loss: 0.5135 - accuracy: 0.8290 - val_loss: 0.7873 - val_accuracy: 0.7585\nEpoch 8/15\n14/74 [====>.........................] - ETA: 4s - loss: 0.4634 - accuracy: 0.8511","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"crmodel.predict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to plot confusion matrix    \ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        \n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    # Predict the values from the validation dataset\nY_pred = crmodel.predictions\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\n#Y_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_test, Y_pred_classes)\n\n \n\n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pruning, Quantizing and Converting to Tflite model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_for_export = tfmot.sparsity.keras.strip_pruning(crmodel.model)\nconverter =tf.lite.TFLiteConverter.from_keras_model(model_for_export)\npruned_tflite_model = converter.convert()\nopen(\"pruned_tflite_model.tflite\", \"wb\").write(pruned_tflite_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"converter1 =tf.lite.TFLiteConverter.from_keras_model(model_for_export)\nconverter1.optimizations = [tf.lite.Optimize.DEFAULT]\nquant_tflite_model = converter1.convert()\nopen(\"quant_tflite_model.tflite\", \"wb\").write(quant_tflite_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the tflite model"},{"metadata":{"trusted":true},"cell_type":"code","source":"pruned_tflite_model_file = 'pruned_tflite_model.tflite'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pruned_interpreter = tf.lite.Interpreter(model_path=str(pruned_tflite_model_file))\npruned_interpreter.allocate_tensors()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quant_tflite_model_file='quant_tflite_model.tflite'\nquant_interpreter = tf.lite.Interpreter(model_path=str(quant_tflite_model_file))\nquant_interpreter.allocate_tensors()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is not the metric that is used for actual accuracy estimation.\nThis is done just to check if quantization of the tflite model reduces accuracy.\nIf yes, then how much?\nIf no, then gg."},{"metadata":{"trusted":true},"cell_type":"code","source":"# A helper function to evaluate the TF Lite model using \"test\" dataset.\ndef evaluate_model(interpreter):\n  input_index = interpreter.get_input_details()[0][\"index\"]\n  output_index = interpreter.get_output_details()[0][\"index\"]\n\n  # Run predictions on every image in the \"test\" dataset.\n  prediction_digits = []\n  for test_image in x_test:\n    # Pre-processing: add batch dimension and convert to float32 to match with\n    # the model's input data format.\n    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n    interpreter.set_tensor(input_index, test_image)\n\n    # Run inference.\n    interpreter.invoke()\n\n    # Post-processing: remove batch dimension and find the digit with highest\n    # probability.\n    output = interpreter.tensor(output_index)\n    digit = np.argmax(output()[0])\n    prediction_digits.append(digit)\n\n  # Compare prediction results with ground truth labels to calculate accuracy.\n  accurate_count = 0\n  for index in range(len(prediction_digits)):\n    if prediction_digits[index] == y_test[index]:\n      accurate_count += 1\n  accuracy = accurate_count * 1.0 / len(prediction_digits)\n\n  return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(evaluate_model(pruned_interpreter))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(evaluate_model(quant_interpreter))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}