{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport keras\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nimport gc","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = \"../input/dermnet/dermnet_images\"","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_dict = {\n    'Acne-and-Rosacea-Photos' : 0,\n    'Cellulitis-Impetigo-and-other-Bacterial-Infections' : 1,\n    'Eczema-and-Atopic-Dermatitis-Photos' : 2,\n    'Nail-Fungus-and-other-Nail-Disease': 3\n}\n\ndef data_loader():\n    #imgs = np.empty(shape=[0, 256, 256, 3])\n    images = []\n    labels = []\n    size = 256,256\n    print(\"Loading...\")\n    for folder in sorted(os.listdir(train_dir)):\n        print(folder)\n        for image in os.listdir(train_dir + \"/\" + folder):\n            temp_img = cv2.imread(train_dir + '/' + folder + '/' + image)\n            temp_img = cv2.resize(temp_img, size)\n            #temp_img = [temp_img]\n            images.append(temp_img)\n            if folder == 'Acne-and-Rosacea-Photos':\n                labels.append(labels_dict['Acne-and-Rosacea-Photos'])\n            elif folder == 'Cellulitis-Impetigo-and-other-Bacterial-Infections':\n                labels.append(labels_dict['Cellulitis-Impetigo-and-other-Bacterial-Infections'])\n            elif folder == 'Eczema-and-Atopic-Dermatitis-Photos':\n                labels.append(labels_dict['Eczema-and-Atopic-Dermatitis-Photos'])\n            elif folder == 'Nail-Fungus-and-other-Nail-Disease':\n                labels.append(labels_dict['Nail-Fungus-and-other-Nail-Disease'])\n\n            #del temp_img, image\n            #gc.collect()\n    images = np.array(images)\n    #images = images.astype('float32')/255.0\n    #imgs = np.append(imgs, images, axis=0)\n    #del images\n    #gc.collect()\n    #imgs = np.array(images)\n    #print(narr.shape)\n    #imgs = np.append(imgs, narr, axis=0)\n    print('Total images : ',images.shape[0])\n    #del images,narrr \n    #gc.collect()\n    \n    templabels = labels\n    \n    labels = to_categorical(labels)\n        \n    X_train, X_test, y_train, y_test = train_test_split(images, labels, train_size=0.9, random_state=42)\n    \n    del images, labels\n    gc.collect()\n    \n    return X_train, y_train, X_test, y_test, templabels","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train, X_test, y_test, labels = data_loader()","execution_count":4,"outputs":[{"output_type":"stream","text":"Loading...\nAcne-and-Rosacea-Photos\nCellulitis-Impetigo-and-other-Bacterial-Infections\nEczema-and-Atopic-Dermatitis-Photos\nNail-Fungus-and-other-Nail-Disease\nTotal images :  4700\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras import regularizers\nfrom keras.optimizers import SGD\nfrom keras.models import load_model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import Callback\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nfrom math import *\nfrom keras import backend\nimport tensorflow as tf\n\nclass CosineAnnealingLearningRateSchedule(Callback):\n    from keras import backend\n\n    # constructor\n    def __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n        self.epochs = n_epochs\n        self.cycles = n_cycles\n        self.lr_max = lrate_max\n        self.lrates = list()\n \n    # calculate learning rate for an epoch\n    def cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n        epochs_per_cycle = floor(n_epochs/n_cycles)\n        cos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n        return lrate_max/2 * (cos(cos_inner) + 1)\n \n    # calculate and set learning rate at the start of the epoch\n    def on_epoch_begin(self, epoch, logs=None):\n        # calculate learning rate\n        lr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n        # set learning rate\n        backend.set_value(self.model.optimizer.lr, lr)\n        # log value\n        self.lrates.append(lr)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install git+https://github.com/qubvel/efficientnet","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!/opt/conda/bin/python3.7 -m pip install --upgrade pip","execution_count":8,"outputs":[{"output_type":"stream","text":"Collecting pip\n  Downloading pip-20.2.3-py2.py3-none-any.whl (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 2.9 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 20.2.2\n    Uninstalling pip-20.2.2:\n      Successfully uninstalled pip-20.2.2\nSuccessfully installed pip-20.2.3\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q tensorflow-model-optimization ","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import Adam, Adadelta, RMSprop\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, SpatialDropout2D, BatchNormalization, LeakyReLU, Activation, Lambda\nfrom keras.layers.advanced_activations import LeakyReLU\n#from keras.initializers import HeNormal\nfrom keras.applications.vgg16 import VGG16\n#from efficientnet.keras import EfficientNetB3\nfrom keras.models import Model\nimport tensorflow_model_optimization as tfmot\nfrom keras.layers.experimental.preprocessing import Rescaling, Normalization, Resizing\nfrom keras.utils import normalize\n#%load_ext tensorboard\n\ndef create_model():\n    input_shape = (256, 256, 3)\n    num_classes = 4\n\n    model = VGG16(weights = 'imagenet', include_top=False, input_shape=input_shape)\n    for layer in model.layers:\n        layer.trainable = False\n    model.get_layer('block5_conv1').trainable = True\n    model.get_layer('block5_conv2').trainable = True\n    model.get_layer('block5_conv3').trainable = True\n    \n    #initializer = HeNormal()\n    flat = Flatten()(model.layers[-1].output)\n    #resize = Resizing(32, 256, 256, 3)(flat)\n    #scale = Rescaling(scale=1./255)(flat)\n    #pre = Lambda(preprocess(flat))\n    class1 = Dense(128)(flat)\n    #drop1 = Dropout(0.25)(class1)\n    act1 = LeakyReLU(alpha=0.1)(class1)\n    class2 = Dense(128)(act1)\n    #drop2 = Dropout(0.25)(class2)\n    act2 = LeakyReLU(alpha=0.1)(class2)\n    output = Dense(num_classes, activation='softmax')(act2)\n    model = Model(inputs=model.inputs, outputs=output)\n\n    print(\"MODEL CREATED\")\n    model.summary()\n    return model","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfg = dict(\n           batch_size=32,    \n           lr_start=5e-5,\n           #lr_max=0.0000125,\n           #lr_min=0.00001,\n           #lr_rampup=5,\n           #lr_sustain=0,\n           #lr_decay=0.8\n        )","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import LearningRateScheduler\n\ndef getLearnRateCallback(cfg):\n    \n    ''' Using callbacks for learning rate adjustments. '''\n    def exp_decay(epoch):\n       initial_lrate = cfg['lr_start']\n       k = 0.11\n       lrate = initial_lrate * exp(-k*epoch)\n       return lrate\n    \n    '''lr_start = cfg['lr_start']\n    lr_max = cfg['lr_max'] * cfg['batch_size']\n    lr_min = cfg['lr_min']\n    lr_rampup = cfg['lr_rampup']\n    lr_sustain = cfg['lr_sustain']\n    lr_decay = cfg['lr_decay']\n\n    def lrfn(epoch):\n        if epoch < lr_rampup:\n            lr = (lr_max - lr_start) / lr_rampup * epoch + lr_start\n        elif epoch < lr_rampup + lr_sustain:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_rampup -\n                                                lr_sustain) + lr_min\n        return lr'''\n\n    lr_callback = LearningRateScheduler(exp_decay, verbose=True)\n    return lr_callback","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\n# Normalizing and Standardizing test data exactly the same as train data\nX_test = X_test.astype('float32')\nX_train = X_train.astype('float32')\nX_test *= X_test/255.0\nX_train *= X_train/255.0\n\nX_test = ((X_test - np.mean(X_test))/(np.std(X_test)))\nX_train = ((X_train - np.mean(X_train))/(np.std(X_train)))\n#scaler = StandardScaler()\n#standardized = scaler.fit_transform(X_test)\n#X_test = scaler.inverse_transform(standardized)\n#normalized = scaler.fit_transform(X_test)\n#X_test = scaler.inverse_transform(normalized)\n\n'''mean = np.mean(X_test, axis=(0, 1, 2))\nbroadcast_shape = [1, 1, 1]\nbroadcast_shape[2] = X_test.shape[3]\nmean = np.reshape(mean, broadcast_shape)\nX_test -= mean\n\nstd = np.std(X_test, axis=(0, 1, 2))\nbroadcast_shape = [1, 1, 1]\nbroadcast_shape[2] = X_test.shape[3]\nstd = np.reshape(std, broadcast_shape)\nX_test /= (std + 1e-6)'''\n\ntrain_datagen = ImageDataGenerator(\n        #rescale=1./255,\n        #featurewise_center=True,\n        samplewise_center=False,\n        #featurewise_std_normalization=True,\n        samplewise_std_normalization=False,\n        zca_whitening=False,\n        rotation_range=40,\n        shear_range = 0.1,\n        zoom_range = 0.3,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        horizontal_flip=True,\n        vertical_flip=True,\n        validation_split=0.2\n)\ntrain_datagen.fit(X_train)\n\ndef preprocess(x):\n    #x = tf.make_tensor_proto(x)\n    #x = tf.make_ndarray(x)\n    x=x.numpy()\n    print(type(x))\n    x *= x/255.0\n    #mean = np.mean(x)\n    #std = np.std(x)\n    #x = (x - mean)/std\n    scaler = StandardScaler()\n    standardized = scaler.fit_transform(x)\n    x = scaler.inverse_transform(standardized)\n    normalized = scaler.fit_transform(x)\n    x = scaler.inverse_transform(normalized)\n    x = tf.convert_to_tensor(x, dtype=tf.float32)\n    return x","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_iterator = train_datagen.flow(X_train, y_train, subset='training', batch_size=cfg['batch_size'])\nval_iterator = train_datagen.flow(X_train, y_train, subset='validation', batch_size=cfg['batch_size'])","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nimport datetime\ndef fit_model():\n    n_epochs = 200\n    #n_cycles = n_epochs/20\n    #ca = CosineAnnealingLearningRateSchedule(n_epochs, n_cycles, 0.001, verbose=1), getLearnRateCallback(cfg)\n    #logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n    callbacks = [tfmot.sparsity.keras.UpdatePruningStep(),\n                 #tfmot.sparsity.keras.PruningSummaries(log_dir=logdir, profile_batch=0),\n                 #getLearnRateCallback(cfg),\n                 EarlyStopping(monitor='loss', patience=3, verbose=1, mode='min'),\n                 ModelCheckpoint('weights_best.hdf5',\n                                 monitor='val_accuracy',\n                                 verbose=1,\n                                 save_best_only=True,\n                                 #save_weights_only=False,\n                                 mode='max'\n                                )\n                ]    \n    pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n        initial_sparsity=0.0, final_sparsity=0.5,\n        begin_step=2000, end_step=4000)\n\n    model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(\n        model, pruning_schedule=pruning_schedule)\n    \n    sgd = SGD(lr=1e-4, momentum = 0.9, nesterov = True)\n    #adam = Adam(learning_rate=1e-4)\n    rmsprop = RMSprop(learning_rate=3e-4)\n    adad = Adadelta()\n    \n    model_for_pruning.compile(loss='categorical_crossentropy',\n            optimizer=Adam(learning_rate=5e-5),\n            metrics=['accuracy'])\n    \n    model_hist = model_for_pruning.fit(train_iterator, epochs = n_epochs, callbacks = callbacks,  validation_data = val_iterator, verbose=1)\n    return model_hist, model_for_pruning","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\ncurr_model_hist, model = fit_model()","execution_count":null,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58892288/58889256 [==============================] - 0s 0us/step\nMODEL CREATED\nModel: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 32768)             0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               4194432   \n_________________________________________________________________\nleaky_re_lu (LeakyReLU)      (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               16512     \n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 516       \n=================================================================\nTotal params: 18,926,148\nTrainable params: 11,290,884\nNon-trainable params: 7,635,264\n_________________________________________________________________\nEpoch 1/200\n106/106 [==============================] - ETA: 0s - loss: 1.3667 - accuracy: 0.4474\nEpoch 00001: val_accuracy improved from -inf to 0.45863, saving model to weights_best.hdf5\n106/106 [==============================] - 67s 636ms/step - loss: 1.3667 - accuracy: 0.4474 - val_loss: 1.2170 - val_accuracy: 0.4586\nEpoch 2/200\n 64/106 [=================>............] - ETA: 22s - loss: 1.2021 - accuracy: 0.4810","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#curr_model_hist.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('weights_best.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras.utils.plot_model(model, \"model.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from tensorboard import notebook\n#notebook.list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#notebook.display(port=6006, height=1000) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=cfg['batch_size'])\nprint(\"\\nTest Accuracy = \", \"{:.2f}%\".format(test_acc*100),\"\\nTest Loss = \" ,\"{:.6f}\".format(test_loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_accuracy(y):\n    if(y == True):\n        plt.plot(curr_model_hist.history['accuracy'])\n        plt.plot(curr_model_hist.history['val_accuracy'])\n        plt.legend(['train', 'test'], loc='lower right')\n        plt.title('accuracy plot - train vs test')\n        plt.xlabel('epoch')\n        plt.ylabel('accuracy')\n        plt.show()\n    else:\n        pass\n    return\n\ndef plot_loss(y):\n    if(y == True):\n        plt.plot(curr_model_hist.history['loss'])\n        plt.plot(curr_model_hist.history['val_loss'])\n        plt.legend(['training loss', 'validation loss'], loc = 'upper right')\n        plt.title('loss plot - training vs vaidation')\n        plt.xlabel('epoch')\n        plt.ylabel('loss')\n        plt.show()\n    else:\n        pass\n    return\n\n\nplot_accuracy(True)\nplot_loss(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.argmax(model.predict(X_test),axis=1)\ny_pred = to_categorical(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint(\"ROC AUC Score: \",roc_auc_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn import metrics\nfrom numpy import argmax\n\nprint(metrics.classification_report(y_test.argmax(axis = 1), y_pred.argmax(axis = 1), digits=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nmatrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\nmatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_for_export = tfmot.sparsity.keras.strip_pruning(model)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\npruned_model = converter.convert()\n\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_pruned_model = converter.convert()\n\nopen(\"quantized_pruned_model_tflite.tflite\", \"wb\").write(quantized_pruned_model)\nopen(\"pruned_model_tflite.tflite\", \"wb\").write(pruned_model)\n\nprint(\"TFlite file generated\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef evaluate_model(interpreter):\n  input_index = interpreter.get_input_details()[0][\"index\"]\n  output_index = interpreter.get_output_details()[0][\"index\"]\n\n  # Run predictions on ever y image in the \"test\" dataset.\n  prediction_digits = []\n  for i, test_image in enumerate(X_test):\n    if i % 1000 == 0:\n      print('Evaluated on {n} results so far.'.format(n=i))\n    # Pre-processing: add batch dimension and convert to float32 to match with\n    # the model's input data format.\n    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n    interpreter.set_tensor(input_index, test_image)\n\n    # Run inference.\n    interpreter.invoke()\n\n    # Post-processing: remove batch dimension and find the digit with highest\n    # probability.\n    output = interpreter.tensor(output_index)\n    digit = np.argmax(output()[0])\n    prediction_digits.append(digit)\n  print('\\n')\n  # Compare prediction results with ground truth labels to calculate accuracy.\n  prediction_digits = np.array(prediction_digits)\n  prediction_digits = to_categorical(prediction_digits)\n  print(metrics.classification_report(y_test.argmax(axis = 1), prediction_digits.argmax(axis = 1), digits=3))\n\nprint('Pruned TFLite test accuracy:')\ninterpreter = tf.lite.Interpreter(model_content=pruned_model)\ninterpreter.allocate_tensors()\ntest_accuracy = evaluate_model(interpreter)\n\nprint('Pruned and Quantized TFLite test accuracy:')\ninterpreter = tf.lite.Interpreter(model_content=quantized_pruned_model)\ninterpreter.allocate_tensors()\ntest_accuracy_quantized = evaluate_model(interpreter)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}